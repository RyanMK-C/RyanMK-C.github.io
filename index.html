<!DOCTYPE html>
<html lang="pt-BR">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Controle Gestual com Hand Tracking</title>
  <style>
    html, body {
      margin: 0;
      padding: 0;
      height: 100%;
      width: 100%;
      font-family: Arial, sans-serif;
      background-color: hsl(210, 50%, 50%);
      overflow: hidden;
      user-select: none;
    }
    #video {
      position: fixed;
      width: 320px;
      height: 240px;
      bottom: 15px;
      right: 15px;
      object-fit: cover;
      border-radius: 12px;
      border: 2px solid lime;
      transform: scaleX(-1); /* Espelha a imagem da câmera */
      z-index: 10;
    }
    #feedbackCanvas {
      position: fixed;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      pointer-events: none;
      z-index: 5;
    }
    #tutorial {
      position: fixed;
      top: 15px;
      right: 15px;
      background: rgba(0, 0, 0, 0.7);
      color: white;
      padding: 15px;
      border-radius: 12px;
      z-index: 20;
      width: 250px;
      font-size: 14px;
      display: flex;
      flex-direction: column;
      gap: 12px;
    }
    #status {
      position: fixed;
      bottom: 10px;
      left: 10px;
      color: white;
      background: rgba(0, 0, 0, 0.6);
      padding: 6px 10px;
      border-radius: 8px;
      font-size: 14px;
      z-index: 20;
    }
  </style>
</head>
<body>
  <video id="video" autoplay playsinline></video>
  <canvas id="feedbackCanvas"></canvas>

  <div id="tutorial">
    <h3>Instruções:</h3>
    <p><strong>Pinça</strong>: Aperte os dedos indicador e polegar para aumentar o brilho.</p>
    <p><strong>Deslizar</strong>: Use o movimento para alterar o brilho da tela.</p>
    <p><strong>Segurar</strong>: Mantenha a mão aberta para interações adicionais.</p>
  </div>

  <div id="status">Carregando câmera...</div>

  <!-- TensorFlow.js + Handpose -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/handpose"></script>

  <script>
    (async () => {
      const status = document.getElementById('status');
      const video = document.getElementById('video');
      const canvas = document.getElementById('feedbackCanvas');
      const ctx = canvas.getContext('2d');
      const tutorial = document.getElementById('tutorial');

      canvas.width = window.innerWidth;
      canvas.height = window.innerHeight;

      // Configuração da câmera
      async function setupCamera() {
        const stream = await navigator.mediaDevices.getUserMedia({
          video: { facingMode: 'user' },
          audio: false,
        });
        video.srcObject = stream;
        return new Promise(resolve => {
          video.onloadedmetadata = () => {
            video.play();
            resolve();
          };
        });
      }

      // Carregar modelo de handpose
      status.textContent = 'Carregando câmera...';
      await setupCamera();
      status.textContent = 'Carregando modelo...';
      await tf.setBackend('webgl');
      const model = await handpose.load();
      status.textContent = 'Modelo carregado. Buscando mão...';

      let currentBrightness = 1;  // Brilho inicial
      document.body.style.filter = `brightness(${currentBrightness})`;

      function clearCanvas() {
        ctx.clearRect(0, 0, canvas.width, canvas.height);
      }

      function drawLandmarks(landmarks) {
        clearCanvas();
        ctx.fillStyle = 'lime';

        const videoWidth = video.videoWidth || 640;
        const videoHeight = video.videoHeight || 480;

        for (const point of landmarks) {
          let x = videoWidth - point[0];  // Espelhar X para combinar com o vídeo espelhado
          let y = point[1];

          // Escalar para o tamanho do canvas
          x = (x / videoWidth) * canvas.width;
          y = (y / videoHeight) * canvas.height;

          ctx.beginPath();
          ctx.arc(x, y, 10, 0, 2 * Math.PI);
          ctx.fill();
        }
      }

      // Detectar a distância entre os dedos para o gesto de pinça
      function distance(p1, p2) {
        return Math.hypot(p1[0] - p2[0], p1[1] - p2[1]);
      }

      let isPinching = false;

      // Detectar mãos e movimentos
      async function detectHands() {
        const predictions = await model.estimateHands(video, true);

        if (predictions.length > 0) {
          const hand = predictions[0];
          const landmarks = hand.landmarks;

          // Desenhar os pontos da mão
          drawLandmarks(landmarks);

          // Detectar gesto de pinça (distância entre os dedos indicador e polegar)
          const dist = distance(landmarks[8], landmarks[4]);
          const PINCH_THRESHOLD = 40;  // Distância mínima para ser considerado pinça

          if (dist < PINCH_THRESHOLD && !isPinching) {
            isPinching = true;
            // Aumentar o brilho quando o gesto de pinça é detectado
            currentBrightness = Math.min(2, currentBrightness + 0.1); // Limita o brilho até 2
            document.body.style.filter = `brightness(${currentBrightness})`;
            status.textContent = `Brilho aumentado: ${currentBrightness.toFixed(2)}`;
          } else if (dist >= PINCH_THRESHOLD && isPinching) {
            isPinching = false;
          }

        } else {
          clearCanvas();
          status.textContent = 'Nenhuma mão detectada';
        }

        requestAnimationFrame(detectHands);
      }

      detectHands();
    })();
  </script>
</body>
</html>
